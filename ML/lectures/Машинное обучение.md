# EDA
**Многомерный анализ (Multivariate)**
	Чтобы увидеть являются ли одна или несколько из них предсказательными для определенного результата. **Предсказательные переменные** являются *независимыми переменными*, а **результат** - *зависимой переменной (выход, целевая переменная)*
**Этапы EDA**
	Этапы EDA могут варьироваться в зависимости от конкретной задачи и данных, но в целом процесс включает следующие *ключевые этапы:*
	1) **Определение цели анализа**.
		Четкое понимание целей анализа и вопросов, на которые необходимо ответить. Это может быть выявление *закономерностей*, поиск *аномалий*, или *подготовка данных*, для дальнейшего моделирования.
	2) **Сбор данных**.
		Сбор данных из различных источников, таких как базы данных, API, файлы (CSV, excel и т.д.) и др.
	3) **Предварительная обработка данных**.
		*Очистка данных:* удаление дубликатов, обработка пропущенных значений, исправление ошибок в данных.
		*Преобразование данных:* изменение формата данных, кодирование категориальных переменных, нормализация или стандартизация числовых признаков.
	4) **Анализ структуры данных.**
		Изучение размеров датасета, типов переменных и их распределений. Это может включать использование методов, таких как **info(), describe()** (библиотека Pandas)
	5) **Визуализация данных.**
		Построение графиков и диаграмм для *визуального представления данных.* Это может включать:
				1) Box Plots;
				2) Scatter Plots;
				3) HeatMaps;
		Визуализация помогает выявить *закономерности, аномалии, взаимосвязи между переменными*
	6) **Анализ статистических характеристик.**
		Вычисление основных статистических показателей, таких как *среднее, медиана, мода, дисперсия, стандартное отклонение*
		Оценка *корреляций* между переменными для понимания их взаимосвязей.
	7) **Выявление аномалий и выбросов.**
		Использование методов визуализации и статистических тестов для обнаружения выбросов или аномальных значений в данных
	8) **Формулирование гипотез.**
		На основе проведенного анализа формулирование гипотез о взаимосвязей между переменными или о поведении целевой переменной
	9) **Подготовка отчётов и выводов.**
		Документирование результатов анализа, создание отчётов с визуализациями и интерпретацией результатов.
	10) **Подготовка к моделированию.**
		На основе полученных знаний подготовка данных для дальнейшего анализа или построения моделей ML. Это может включать *выбор признаков, создание новых переменных* или дальнейшую *очистку данных.*
**Ключевые идеи:**
	**EDA** - *итеративный процесс*, который может потребовать возвращения к предыдущим этапам по мере получения новых инсайтов из данных.
	**Основная цель EDA** - лучше *понять* данные и *подготовить* их для последующего анализа или моделирования
**Размер и размерность данных.**
	**Размер данных** относится к числу объектов данных
	**Размерность данных** относится к числу атрибутов (признаков)
**Типы атрибутов (признаков)**
	![[Pasted image 20250214155256.png]]
**Типы данных**
	*Категориальные (или качественные) признаки* из неупорядоченного множества:
		1) бинарные: {0, 1}, {True, False}
		2) номинальные: \<city> {Vancouver, Burnaby, и т.д.}
		3) порядковые: 
	*Числовые (или количественные) признаки* из упорядоченного множества:
		4) числа
		5) непрерывные/действительные
	**Номинальные данные (Nominal Data)**
		*Возможные значения:* метки или названия вещей, категории. Например: пол, специальность, штат, цвет.
		Описывают характеристику *качественно,* а значения *не имеют порядка.*
		*Не количественные,* над ними *нельзя выполнять арифметические операции.*
		Можно вычислить:
			1) частота значений и наиболее частое значение.
		**Частота (Frecuency)**
			Номинальные и порядковые атрибуты обычно описывают частотами.
			*Частота значения* - это количество раз, когда значения встречаются в наборе данных.
			Иногда используется дробь или процент времени, когда значение появляется.
		Сбор данных посредством определения *частоты каких-либо вещей* либо строите *частоту в контексте доли от общего количества значений*
		*Двоичный атрибут:* частный случай номинальных значений **true/false, Pass/Fail, 0/1**
		*Симметричный:* оба символа имеют одинаковый вес, например, пол.
		*Асимметричный:* оба символа не одинаково важны, например, сдал/не сдал
		Для *визуализации номинальных данных* часто используют *круговую диаграмму, гистограмму*
	**Порядковые данные (Ordinal Data)**.
		Значения имеют *осмысленный порядок.*
			*Оценки:* A, B, C, D
			*Размеры порций:* маленькие, средние, большие
			*Оценки:* плохо, средне, отлично
		*Количественной разницей между двумя уровнями не обнаружено*
		**A** выше/лучше **B**, но *невозможно количественно определить*, насколько **A** выше **B** или является разница между ними такая же как между **B** и **C**
		Можно вычислить: 
			2) частота значений и наиболее частое значение
			3) среднее значение (mean)
	**Числовые признаки (Numeric Attributes)**
		*Количественные* данные, которые можно *измерить.*
		Можно *количественно определить разницу между двумя значениями.* Например: температуру, возраст, рост и т.д.
		Можно вычислить:
			1) частота значений и наиболее частое значение
			2) среднее значение (mean)
			3) среднее значение атрибута (average)
**Как можно преобразовать *категориальные признаки* в *числовые*?**
	![[Pasted image 20250214162004.png]]
	![[Pasted image 20250214162054.png]]
	![[Pasted image 20250214162736.png]]
**Основы описательной статистики (Statistical EDA)**
	**Описательная статистика** - раздел статистической науки, в рамках которого изучаются методы систематизации, описания и представления основных свойств данных.
	1) Оценки, дающие общую картину данных;
	2) Сводные статистические данные - это числа суммирующие свойства данных;
	3) Типичные свойства данных;
	4) *Разброс (spread) и распределение (distribution)* значений;
	5) *Зависимости* и *корреляции* между переменными.
	**Меры центральной тенденции**
		**Мера центральной тенденции* (Central Tendency)** - число, характеризующее выборку по уровню выраженности измеренного признака; описывает место концентрации или середину данных
		Три меры центральной тенденции:
			1) **Среднее** (среднее арифметическое всех значений)
			2) **Медиана** (серединное значение)
			3) **Мода** (наиболее частое наблюдение)
		Разница между медианой и средним значение существует из-за **рабостности (выбросоустойчивости)**
		Данные *распределены вокруг этого "центра"*
		Вычисляется для каждого атрибута
		Три распространенных типа местоположений:
			1) Среднее
			2) Мода
			3) Медиана
		*Эти меры не дают информации относительно*
			1) экстремальных значений в распределении данных
			2) разброса данных
		![[Pasted image 20250214164428.png]]
**Проблема выбросов**
	Если в данных есть *выбросы* - значения, которые гораздо выше или ниже остальных, - это может *негативно повлиять на среднее значение.*
	**Выброс** - наблюдение, которое сильно выделяется из общей картины для выборки.
	Таким образом, среднее значение на робастно, а медиана - напротив, выбросоустойчива.
	Выбросы могут отражать интересные события или *ошибки* в датасете, поэтому важно уметь определять их наличие.
	*Сравнение медианы и моды* - один из способов определить наличие выбросов, хотя *визуализация* обычно позволяет это сделать быстрее.
	Поэтому ещё один вопрос, который необходимо задать: *насколько сильно разбросаны друг относительно друга значения?*
	Для ответа на такой вопрос существуют **МЕРЫ РАЗБРОСА**.
**Меры разброса (изменчивости)**
	1) max;
	2) min;
	3) Range (Размах) := max - min;
	4) Midrange := среднее от min и max;
	5) Inter-Quartile Range (Межквартильный размах);
	6) Low Spear Mid-spread High Spread;
	7) Variance (Дисперсия) и Standart Deviaton (Стандартное отклонение).
	**Размах и квартильный размах**
		Размах (Range) - разность максимального и минимального значений вариоционного ряда.
		Характеристика разброса данных в выборке - **квартильный (или межквартильный) размах**. Он основывается на понятии *квартилей*.
		**Квартиль** - значение, которые делят выборку на 4 равные (по количеству объектов) части.
		Под *квартилями* понимают значения $Q_{1}, Q_{2}, Q_{3}$ которые делят вариационный ряд на четыре равные части.
		**Размах квартилей** - разница между $Q_{3}\ и\ Q_{1}$
	![[Pasted image 20250221153713.png]]
**Классификация выбросов**
	Классифицируют два вида выбросов: *умеренные* и *экстримальные*.
	*Умеренными* называют выбросы, которые удалены ниже первой квартили или выше третьей от 1.5IQR, но не более, чем на 3IQR. *Экстримальные выбросы* удалены ниже первой квартили или выше третьей более, чем на 3IQR. Схема определения выбросов приведена на рисунке. Определение выбросов крайне важно на этапе подготовки данных и позволяет избавиться от значений, достоверность которых соминтельна.
	![[Pasted image 20250221154204.png]]
**Метод межквартильных размахов (IQR)**
	Чтобы справиться с выбросами, можно либо *отбросить выбросы*, либо *заменить выбросы*, используя *метод межквартильных размахов (IQR)*.
	В EDA вычислить IQR для выявления закономерностей путем нахождения разницы между 25-м и 75-м процентилями данных.
	**Процентиль** - делит данные на 100 частей.
	IQR используется для выявления выбросов путём определения пределов для значений выборки , которые являются фатором k IQR. Общим значением для фактора k является значения 1.5.
	![[Pasted image 20250221154551.png]]
**Дисперсия**
	**Дисперсия** - измеряет *отклонение* значений относительно *среднего*.
	$$\sigma^2 = \frac{\Sigma_{i=1}^n(x_{i}-x)^2}{n}$$
	**Дисперсия** - это среднеквадратичное значение отклонений каждой из величин от среднего
	**Стандартное отклонение**
		**Стандартное отклонение (st-dev)** - мера разброса точек данных. Оно измеряет изменчивость путём вычисления среднеквадратичного расстояния между наблюдениями и средним значением из набора данных.
		$$\sigma = \sqrt{\frac{\Sigma_{i=1}^n(x_{i}-x)^2}{n}}$$
		**Стандартное отклонение** измеряет *отклонение* значений *относительно среднего значения*.
		На практике чаще используется именно *стандартное отклонение*, т.к. оно выражает *изменчивость в исходных единицах измерения признака*
**Меры центральной тенденции**
	Если *распределение симметричное*, то мода, медиана и среднее совпадают.
	Чем *больше отклонение от симметричности*, тем больше расхождение между значениями этих мер центральной тенденции. По этому расхождению можно судить о том, насколько симметрично или асимметрично распределение.
**Нормализация данных**
	**Нормализация данных** - процесс преобразования значений признаков данных так, чтобы они находились в одном диапазоне, что особенно важно, поскольку исходные диапазоны признаков могут сильно варьироваться.
	Нормализация предполагает изменение исходного диапазона значений признака на диапазон с заданным минимум и максимум, например $[0, 1]$ или $[-1, 1]$. Такой подход позволяет нормализовать дисперсию между признаками в наборе данных, которая в противном случает может оказаться преувеличенной вследствии влияния того или иного фактора.
	$$X_{norm} = \frac{X-X_{min}}{X_{max}-X_{min}}$$
	![[Pasted image 20250221160042.png]]
**Стандартизация данных (Z-преобразование)**
	Более эффективный метод подчёркивания *высоких* или *низких значений признака* - **стандартизация**, при которой данные преобразуются так, чтобы иметь *среднее значение 0 и стандартное отклонение 1*. Это будет означать, что *чрезвычайно высокое или низкое значение* будет выражено в виде трех или более стандартный отклонений от среднего.
	Стандартизация полезна когда:
		1) данные имеют *разные размеры* и *единицы измерения*;
		2) выбросы важны и должны быть учтены, но важно заставить модель сосредоточиться на относительных различияхю
	![[Pasted image 20250221160419.png]]
	![[Pasted image 20250221160520.png]]
	Эта формула эффективно маштабирует данные, *центрируя их вокруг 0 и корректируя разброс на основе стандаратного отклонения*.
	**Стандартизацию** рекомендуют выполнять при подготовке данных для последующего использования *регрессии, машин опорных векторов (SVM), метод k-ближайших соседей (k-NN)* и выполнение *анализа главных компонент (PCA)*.
**Числовые сводки данных**
	**Меры центральной тенденции (Measure of Central Tendency)**: вычисляются, чтобы дать **центр**, вокруг которого распределены измерения в данных.
	**Меры вариации или изменчивости (Measure of Variation)**: описывают *разброс данных (изменчивость значений)* или насколько далеко измерения находятся от центра (среднего значения).
**Работа с дисперсией**
	**Что показывает дисперсия?**
		**Дисперсия в статистике** - это мера, которая показывает разброс между результатами. Если все они *близки к среднему*, **дисперсия низкая**. Если *результаты сильно различаются* - **дисперсия высокая**.
		Если говорить о всей выборке, дисперсия показывает, *насколько разнородны результаты*. Например, в *1 группе* почти все - шатены; во *2 группе* половина - шатены, а остальные блондины, рыжие.
	**Кто работает с дисперсией?**
		![[Pasted image 20250221161708.png]]
	**Связь с другими показателями**
		1) Среднее арифметическое;
		2) Стандартное отклонение;
		3) Смещение;
		4) Ошибка прогнозирования
**Разреженность в данных (Sparsity in Data)**
	Если большинство значений признаков отсутствуют, то данные называются **РАЗРЕЖЕННЫМИ**.
	Отсутствующие значения могут быть представленные как NAN, пусто, -, 0.
	Это может быть проблемой для многих стат. методов
	Если пропусков у признака-столбца слишком много (более 70%), то такой признак удаляют!
**Обработка пропущенных значений**
	![[Pasted image 20250221163008.png]]
	![[Pasted image 20250221163011.png]]
**Визуализация данных**
	**Визуализация данных** позволяет понять паттерны, тренды, взаимосвязи в данных через графику и диаграммы.
	![[Pasted image 20250221163241.png]]
# Машинное обучение. Контролируемое обучение (Supervised Learning (SL))
**Модель алгоритма**
	Решить задачу ML означает разработать алгоритм или модель алгоритма, зависящего от параметров и позволяющих определить значение метки класса (Y) для нового объекта (x).
	Моделью алгоритма a называется параметрическое семейство $g:X Y \ или\ g(x,\theta) $
	Процесс подбора оптимальной функции g и оптимальных параметров тета по обучащей выборке называют настройки (fitting, tuning) или обучение (traning) алгоритма a.
	**Признаковое описание объекта. Признаковое пространство.** Размерность признакового пространства определяется количеством признаков.
	**Признаки** объекта x можно записать в виде вектора $$x_{1},...,x_{d}$$
	**Обучающая выборка.**
		Матрица X
**О терминах, которые используются в ML**
	Пусть по характеристикам клиента (пол, возраст, средний доход, рейтинг кредитной истории и т.д.) нужно предсказать, вернёт клиент кредит или не вернёт.
	**Целевая переменная (target, y)**, т.е. величина, которую хотим предсказать - это число.
	Характеристики клиента, а именно, его пол, возраст, доход и т.д. называются **признаками (features)**.
	Сами же клиенты - сущности, с которыми работаем в этой задаче - называются **объектами (objects)**.
	**Обучающая выборка (traning, set).** Конечный набор объектов x, для которых известны значения целевой переменной y, т.е. все данные с *известными ответами* называются **обучающейся выборкой.**
	**Обучающая выборка** может быть разбита на несколько частей:
		1)**Тренировочная выборка:** используется непосредственно для обучения модели;
		2) **Валидационная выборка:** используется для настройки гиперпараметров и проверки качества модели в процессе обучения;
		3) **Тестовая выборка:** используется для окончательной оценки производительности модели на данных, которые не были использованы в процессе обучения.
**Оценка обобщающая способности модели ML**
	Одна из важных характеристик моделей ML - способность к обобщению (насколько хорошо модель работает на новых данных).
	Данные, на которых оцениваются качество модели, берут из первоначального датасета.
	Существует несколько подходов как *отщепить* часть данных.
	Рассмотрим два самых популярных из них:
		Отложенная выборка (held-out/hold-out set);
		Кросс-валидация (cross-validation).
**Обучение с учителем**
	В **обучении с учителем** набор данных организован как коллекция размеченных образцов  $$\{(x_{i},y_{i})\}^N_{i=1}$$ каждый элемент $x_{i}$ из N - вектор признаков.
	**Вектор признаков** - вектор, в котором каждое измерение $j=1,...,D$ содержит значение, описывающее некоторую характеристику образца.
	Признак обозначается как $X_{j}$
	**Метка y_{i}** м.б. элементом конечного множества классов {1,2,...,C} вещественным числом или более сложной структурой (вектор, матрица, дерево или граф).
	В области ML модель реализует *алгоритм ML.*
	В терминологии Scikit-Learn модели называются **оценщиками (estimator)**
	**Цель алгоритма обучения с учителем** - на основе набора векторов данных создать **модель**, которая принимает *вектор признаков x на входе* и возвращает информацию, которая позволяет определить *метку для этого вектора признаков.*
	![[Pasted image 20250307155053.png]]
	**Весь процесс ML делится на 2 этапа:** **ОБУЧЕНИЕ АЛГОРИТМА** и **ПРИМЕНЕНИЕ АЛГОРИТМА**
	**Общая схема решения задачи обучения с учителем:**
	1) Загрузить данные, выполнить их первичный анализ, выявить целевой признак;
	2) Предварительная обработка данных и EDA, разделить данные на обучающую и тестовую выборки;
	3) Построить модель с использованием различных множеств значений гиперпараметров;
	4) Оценить качество построенных моделей с использованием метрик; выбор лучшей модели;
	5) Применить модель для получения предсказаний.
	При решении задач классического ML наборы данных X и y как правило делятся ещё 2 части (как правило 80:20):
	$X_{train},X_{test}$ и $y_{train},y_{test}$
	![[Pasted image 20250307160252.png]]
	![[Pasted image 20250307160632.png]]
	![[Pasted image 20250307160834.png]]
	**Понятие гиперпараметров модели**
		Существуют два типа параметров ML:
			1) вычисляемые оценщиком в ходе своего обучения на основании предоставленных вами данных;
			2) задаваемые заранее при создании объекта оценщика Scikit-Learn, представляющего модель.
		Параметры, задаваемые заранее, называются **ГИПЕРПАРАМЕТРАМИ**
		**Гиперпараметр** - параметр, значение которого задаётся до начала обучения модели и не изменяется в процессе обучения. У модели может не быть гиперпараметра.
		**Параметр модели** - параметр, который изменяется и оптимизируется в процессе обучения модели, итоговое значение параметра - результат обучения модели.
**Основные проблемы ML**
	Недостаточный объём данных;
	Пропуски в данных;
	Переобучение (Overfitting);
	Недообучение (Underfitting).
	**Переобучение** - нежелательное явление, возникающее при решении задач обучения по прецедентам, когда вероятность ошибки обученного алгоритма на объектах тестовой выборки оказывается существенно выше, чем средняя ошибка на обучающей выборке. Переобучение возникает при использовании избыточно сложных моделей.
	**Недообучение** - нежелательное явление, возникающее при решении задач обучения по прецедентам, когда алгоритм обучения не обеспечивает достаточно малой величины средней ошибки на обучающей выборке. Недообучение возникает при использовании недостаточно сложных моделей.
**Линейные модели. Задача регрессии**
	Под задачей регрессии подразумевают определение математической модели (функции регрессии), устанавливающей функциональную связь между зависимой переменной и группой независимых переменных с учётом ошибки модели.
	**Регрессия в ML** - это задача ML с учителем, целью которого выступает установление взаимосвязей между различными переменными.
	**Регрессионный анализ**
		**Регрессионный анализ** - статистический инструмент для установления модели отношений между двумя переменными.
		Одна из этих переменных - предикторная переменная, значение которой собирается в ходе экспериментов.
		Другая переменная - переменной ответа, значение которой получено из переменной предиктора.
		В зависимости от форм мат. модели, которая используется для аппроксимации данных, регрессия бывает линейной и нелинейной.
		**ЛР** представляет собой статистический метод, который моделирует линейную зависимость между зависимой переменной и одной или несколькими независимыми переменными.
		Математически линейная зависимость представляет прямую линию, когда она изображена в виде графика.
		Нелинейная регрессия описывает нелинейные отношения между зависимой и независимой переменными.
		Нелинейное отношение, где показатель степени любой переменной не равен 1, создаёт кривую.
**Ключевые термины**
	**Отклик (pesponse)**
		Переменная, которую пытаемся предсказать.
		*Синонимы*: зависимая переменная, Y-переменная, цель, исход.
	**Независимая переменная (independent variable)**
		Переменная, которая используется для предсказания отклика.
		*Синонимы*: независимая переменная, X-переменная, предиктор, признак, атрибут.
	**Запись (record)**
		Вектор, который состоит из значений предикторов и значений исхода для индивидуального элемента данных или случая.
		*Синонимы*: строка, случай, прецедент, образец, экземпляр, пример.
	**Пересечение (intercept)**
		Пересечение регрессионной прямой, т.е. предсказанное значение, когда X=0.
		*Синонимы*: точка пересечения.
	**Коэффициент регрессии (regression coefficient)**
		Наклон регрессионной прямой.
		*Синонимы*: наклон, оценка параметров, веса.
	**Подогнанные значения (fitted values)**
		Оценки полученные из регрессионной прямой.
		*Синонимы*: предсказанные значения.
	**Остатки (residuals)**
		Разница между наблюдаемыми значениями и подогнанными значениями.
		*Синонимы*: ошибки.
	**Наименьшие квадраты (least squares)**
		Метод подгонки регрессии путём минимизации суммы квадратов остатков.
		*Синонимы*: обычный метод наименьших квадратов или обычный МНК.
**Постановка задачи регрессии**
	![[Pasted image 20250314152339.png]]
	![[Pasted image 20250314153037.png]]
**Формальное определение линейной модели**
	Модель регрессии называется линейной, если значение предсказываемого признака Y вычисляется как сума известных признаков $X_{1}, X_{2},...,X_{m}$, взятых с некоторыми коэффициентами.
	$$y'=w_{1}x_{1}+w_{2}x_{2}+,...,+w_{m}x_{m}+w_{0}$$
	Линейная комбинация признаков для прогнозирования целевого значения.
	Задача заключается в нахождении оптимальных весов (коэффициентов).
	Парная и множественная регрессии помогают определить веса для каждой независимой переменной, которые минимизируют разницу между прогнозируемыми и наблюдаемыми значения зависимой переменной.
**Минимизация эмпирического риска**
	Чаще всего при построении алгоритмов обучения используется метод минимизации эмпирического риска (средней ошибки алгоритма на обучающей выборке).
	Его суть состоит в том, чтобы для текущей модели подобрать алгоритм, минимизирующий значение средней ошибки на данной обучающей выборке.
	Этот подход может включать в себя разные функции потерь в зависимости от задачи ML.
	Функция потерь представляет собой метрику, которая в рамках оценки разности статистических заданных и полученных значений отражает отклонения результатов моделирования от реальных данных.
	Метод наименьших квадратов (МНК) - специфический случай минимизации эмпирического риска.
**Линейная регрессия МНК (Ordinary Least Squares - OLS)**
	Метод наименьших квадратов (МНК) - метод нахождения оптимальных параметров линейно регрессии, таких, что сумма квадратов ошибок минимальна.
	Метод заключается в минимизации евклидова расстояния между двумя векторами - вектором восстановленных значений зависимой переменной и вектором фактических значений зависимой переменной.
	МНК важен в алгоритмах ЛР, потому что он позволяется:
		1) Минимизировать суммарные ошибки прогнозирования. Это помогает получить наилучшую линейную модель.
		2) Находить лучшую линию (или кривую), чтобы максимально точно описать связь между двумя переменными.
**Метрики качества модели ML**
	В задачах ML для оценки качества моделей и сравнения различных алгоритмов используются метрики.
	Метрика - функция для определения расстояния между двумя элементами множества.
	Большинство метрики качества основано на понятии ошибки прогноза.
	- MAE
	- MSE
	- RMSE
	- MAPE
	- SMAPE
	- $R^2$
	Функция потерь (функция ошибки) - мера колич6ства ошибок, которые линейная регрессия делает на наборе данных.
	Хотя есть разные функции потерь, все они вычисляют расстояние между предсказанным значением y(x) и его фактическим значением.
	**Коэффициент детерминации $R^2$**
		Общая метрика регрессии - коэффициент детерминации.
		Значение находится в $[0;1]$. Это доля дисперсии зависимой переменной, объясняемая рассматриваемой моделью зависимости, т.е. объясняющими переменными.
		КД показывает количество объясненной дисперсии.
		КД численно показывает, сколько процентов разброса данных объяснила модель. Чем ближе к 1, тем лучше (но не всегда).
	**Среднеквадратичная ошибка**
		Метрика MSE вычисляется как сумма квадратов прогноза, деленная на количество точек тестовой выборки.
		Чем ближе прогнозы модели к наблюдениям, тем меньше будет MSE.
	**Корень среднеквадратичной ошибки**
		Метрика RMSE показатель, вычисляет корень из MSE.
		Чем ниже RMSE, тем лучше данная модель может "соответствовать" набору данных.
	**Средняя абсолютная ошибка**
		Метрика MAE вычисляет среднюю абсолютную разницу между прогнозом и фактом в наборе данных.
		Чем ниже MAE, тем лучше модель соответствует набору данных.
	**Средняя абсолютная ошибка в процентах**
		Метрика MAE в процентах.
	**Симметричная средняя абсолютная ошибки в процентах**
		SMAPE - мера точности, основанная на процентных ошибках. Т.е. абсолютная разность между наблюдаемым и предсказанным значениями делится на полу-сумму их модулей.
**Модели регуляризации линейной регрессии: Lasso, Ridge, ElasticNet**
	**Регуляризация** в статистике, ML, теории обратных задач - метод добавления некоторых доп. ограничений к условию с целью решить некорректно поставленную задачу или предотвратить переобучение. Чаще всего эта информация имеет вид штрафа за сложность модели.
	Переобучению в большинстве случаем проявляется в том, что итоговые модели имеют слишком большие значения параметров. Соответственно, необходимо добавить в целевую функцию штраф.
	**Основные виды регуляризации**
		- L1
		- L2
		- ElasticNet
	**Мультиколлинеарность**
		Другая проблема, о которой следует знать, - это мультиколлинеарность.
		Если столбцы имеют высокую корреляцию, это может затруднить интерпретацию коэффициентов.
		На модель это обычно не влияет, влияет только на смысл коэффициента.
	**L1 (Lasso)**
		![[Pasted image 20250314162000.png]]
	**L2 (Ridge)**
		![[Pasted image 20250314162609.png]]
	**Норма**
		В математике норма вектора - его длина.
		В регрессионном анализе, чтобы соответствовать линейной модели, нужна мера несоответствия.
		Длина вектора в ML - ошибка в каждом обучающих данных (длина вектора, характеризующая разницу между истинным значением целевой переменной и значением, спрогнозированным моделью ML). Нужно измерить длину ошибки.
	**Отличие L2 от МНК**
		Коэффициенты $w_{1},w_{2},...$ выбираются согласно МНК, но далее корректируются так, чтобы каждый вес был близок к 0.
		Это означает, что каждый признак должен иметь как можно меньше влияние на результат.
	**Отличие L1 от L2**
		Основное отличие Lasso от Ridge - L1 может приводить обращение некоторых независимых переменных в ноль, тогда как L2 уменьшает их до значений, близких к нулю.
	**ElasticNet**
		Данный тип регуляризации учитывает эффективность обоих методов регуляризации L1 и L2, применяется в тех случаях, когда важно важно сохранить корреляционную связь переменных и не допустить зануление коэффициентов в регрессионной модели.
		![[Pasted image 20250314163912.png]]
